{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "os.makedirs(\"./recon\", exist_ok=True)\n",
    "\n",
    "# Anomaly number\n",
    "ANOMALY_TARGET = 5\n",
    "\n",
    "parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
    "                    help='input batch size for training (default: 128)')\n",
    "parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--no-mps', action='store_true', default=False,\n",
    "                        help='disables macOS GPU training')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if args.cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "# Training dataset excluding anomaly target numbers\n",
    "train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_mask = (train_dataset.targets != ANOMALY_TARGET)\n",
    "train_dataset.data = train_dataset.data[train_mask]\n",
    "train_dataset.targets = train_dataset.targets[train_mask]\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "all_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "# anomaly target numeric-only dataset\n",
    "anomaly_dataset = datasets.MNIST('../data', train=False, download=True, transform=transforms.ToTensor())\n",
    "anomaly_mask = (anomaly_dataset.targets == ANOMALY_TARGET)\n",
    "anomaly_dataset.data = anomaly_dataset.data[anomaly_mask]\n",
    "anomaly_dataset.targets = anomaly_dataset.targets[anomaly_mask]\n",
    "anomaly_loader = torch.utils.data.DataLoader(anomaly_dataset, batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "all_anomaly_loader = torch.utils.data.DataLoader(anomaly_dataset, batch_size=1, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "            \n",
    "        # Reconstruction for training data\n",
    "        if batch_idx == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n], recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(), 'recon/recon_train' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "    return - (train_loss / len(train_loader.dataset))\n",
    "\n",
    "\n",
    "def anomaly(epoch):\n",
    "    model.eval()\n",
    "    anomaly_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(anomaly_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            anomaly_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "\n",
    "            # Reconstruction for anomaly data\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n], recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(), 'recon/recon_anomaly' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    anomaly_loss /= len(anomaly_loader.dataset)\n",
    "    print('====> anomaly set loss: {:.4f}'.format(anomaly_loss))\n",
    "\n",
    "    return - anomaly_loss\n",
    "\n",
    "def plot_roc():\n",
    "    y_true = np.concatenate([np.zeros(len(train_dataset)), np.ones(len(anomaly_dataset))])\n",
    "    y_score = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(all_train_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            train_score_loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            train_score_loss = train_score_loss.cpu()\n",
    "            y_score.append(np.round(train_score_loss, 1).detach().numpy())\n",
    "        for i, (data, _) in enumerate(all_anomaly_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            anomaly_score_loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            anomaly_score_loss = anomaly_score_loss.cpu()\n",
    "            y_score.append(np.round(anomaly_score_loss, 1).detach().numpy())\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('FPR: False positive rate', fontsize=13); plt.ylabel('TPR: True positive rate', fontsize=13)\n",
    "    plt.grid()\n",
    "    plt.savefig('./roc'+str(ANOMALY_TARGET)+'.png')\n",
    "    plt.close()\n",
    "    print(\"AUC:\" + str(np.round(auc, 2)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.set_xlabel('Epoch', fontsize=15); ax1.set_ylabel('ELBO', fontsize=15)  \n",
    "    train_elbo_list = [];anomaly_loss_list = []\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        avg_train_elbo = train(epoch);avg_anomaly_elbo = anomaly(epoch)\n",
    "        train_elbo_list.append(avg_train_elbo);anomaly_loss_list.append(avg_anomaly_elbo)\n",
    "\n",
    "    # Plot ELBO\n",
    "    ax1.plot(np.arange(args.epochs), train_elbo_list, color=\"blue\", label=\"ELBO_Train\")\n",
    "    ax1.plot(np.arange(args.epochs), anomaly_loss_list, color=\"red\", label=\"ELBO_Anomaly\")\n",
    "    fig1.savefig('./elbo.png')\n",
    "    plt.close()\n",
    "\n",
    "    # Plot ROC\n",
    "    plot_roc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Mar 13 2023, 10:26:41) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
