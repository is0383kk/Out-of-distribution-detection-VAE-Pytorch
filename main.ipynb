{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/is0383kk/AnomalyDetection_VAE/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ESzj2DO1ttg-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f045a09f-f6ae-4499-eee5-4f84d592eb96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 100949408.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 64331223.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 25289489.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 23232352.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/54579 (0%)]\tLoss: 550.656433\n",
            "Train Epoch: 1 [1280/54579 (2%)]\tLoss: 292.733917\n",
            "Train Epoch: 1 [2560/54579 (5%)]\tLoss: 229.428635\n",
            "Train Epoch: 1 [3840/54579 (7%)]\tLoss: 213.753387\n",
            "Train Epoch: 1 [5120/54579 (9%)]\tLoss: 209.712814\n",
            "Train Epoch: 1 [6400/54579 (12%)]\tLoss: 205.545929\n",
            "Train Epoch: 1 [7680/54579 (14%)]\tLoss: 200.870071\n",
            "Train Epoch: 1 [8960/54579 (16%)]\tLoss: 204.278229\n",
            "Train Epoch: 1 [10240/54579 (19%)]\tLoss: 190.554382\n",
            "Train Epoch: 1 [11520/54579 (21%)]\tLoss: 188.046509\n",
            "Train Epoch: 1 [12800/54579 (23%)]\tLoss: 181.019241\n",
            "Train Epoch: 1 [14080/54579 (26%)]\tLoss: 174.194916\n",
            "Train Epoch: 1 [15360/54579 (28%)]\tLoss: 172.269974\n",
            "Train Epoch: 1 [16640/54579 (30%)]\tLoss: 165.651031\n",
            "Train Epoch: 1 [17920/54579 (33%)]\tLoss: 159.578690\n",
            "Train Epoch: 1 [19200/54579 (35%)]\tLoss: 160.503647\n",
            "Train Epoch: 1 [20480/54579 (37%)]\tLoss: 148.997757\n",
            "Train Epoch: 1 [21760/54579 (40%)]\tLoss: 147.008011\n",
            "Train Epoch: 1 [23040/54579 (42%)]\tLoss: 150.418304\n",
            "Train Epoch: 1 [24320/54579 (44%)]\tLoss: 146.782700\n",
            "Train Epoch: 1 [25600/54579 (47%)]\tLoss: 150.677567\n",
            "Train Epoch: 1 [26880/54579 (49%)]\tLoss: 150.558777\n",
            "Train Epoch: 1 [28160/54579 (52%)]\tLoss: 146.430878\n",
            "Train Epoch: 1 [29440/54579 (54%)]\tLoss: 137.887527\n",
            "Train Epoch: 1 [30720/54579 (56%)]\tLoss: 146.770599\n",
            "Train Epoch: 1 [32000/54579 (59%)]\tLoss: 145.002411\n",
            "Train Epoch: 1 [33280/54579 (61%)]\tLoss: 138.842972\n",
            "Train Epoch: 1 [34560/54579 (63%)]\tLoss: 135.920395\n",
            "Train Epoch: 1 [35840/54579 (66%)]\tLoss: 139.247803\n",
            "Train Epoch: 1 [37120/54579 (68%)]\tLoss: 133.576782\n",
            "Train Epoch: 1 [38400/54579 (70%)]\tLoss: 136.465546\n",
            "Train Epoch: 1 [39680/54579 (73%)]\tLoss: 137.162552\n",
            "Train Epoch: 1 [40960/54579 (75%)]\tLoss: 135.344666\n",
            "Train Epoch: 1 [42240/54579 (77%)]\tLoss: 141.359772\n",
            "Train Epoch: 1 [43520/54579 (80%)]\tLoss: 133.091980\n",
            "Train Epoch: 1 [44800/54579 (82%)]\tLoss: 129.666183\n",
            "Train Epoch: 1 [46080/54579 (84%)]\tLoss: 131.517471\n",
            "Train Epoch: 1 [47360/54579 (87%)]\tLoss: 130.214447\n",
            "Train Epoch: 1 [48640/54579 (89%)]\tLoss: 129.744293\n",
            "Train Epoch: 1 [49920/54579 (91%)]\tLoss: 129.133591\n",
            "Train Epoch: 1 [51200/54579 (94%)]\tLoss: 132.320938\n",
            "Train Epoch: 1 [52480/54579 (96%)]\tLoss: 130.254593\n",
            "Train Epoch: 1 [53760/54579 (98%)]\tLoss: 128.659454\n",
            "====> Epoch: 1 Average loss: 165.8569\n",
            "====> anomaly set loss: 165.4973\n",
            "Train Epoch: 2 [0/54579 (0%)]\tLoss: 129.114655\n",
            "Train Epoch: 2 [1280/54579 (2%)]\tLoss: 132.736725\n",
            "Train Epoch: 2 [2560/54579 (5%)]\tLoss: 129.612595\n",
            "Train Epoch: 2 [3840/54579 (7%)]\tLoss: 130.808517\n",
            "Train Epoch: 2 [5120/54579 (9%)]\tLoss: 125.865746\n",
            "Train Epoch: 2 [6400/54579 (12%)]\tLoss: 124.953667\n",
            "Train Epoch: 2 [7680/54579 (14%)]\tLoss: 126.745247\n",
            "Train Epoch: 2 [8960/54579 (16%)]\tLoss: 120.813599\n",
            "Train Epoch: 2 [10240/54579 (19%)]\tLoss: 127.001091\n",
            "Train Epoch: 2 [11520/54579 (21%)]\tLoss: 128.328583\n",
            "Train Epoch: 2 [12800/54579 (23%)]\tLoss: 126.709900\n",
            "Train Epoch: 2 [14080/54579 (26%)]\tLoss: 126.141678\n",
            "Train Epoch: 2 [15360/54579 (28%)]\tLoss: 119.452682\n",
            "Train Epoch: 2 [16640/54579 (30%)]\tLoss: 125.365906\n",
            "Train Epoch: 2 [17920/54579 (33%)]\tLoss: 124.158348\n",
            "Train Epoch: 2 [19200/54579 (35%)]\tLoss: 116.968910\n",
            "Train Epoch: 2 [20480/54579 (37%)]\tLoss: 123.938919\n",
            "Train Epoch: 2 [21760/54579 (40%)]\tLoss: 120.600052\n",
            "Train Epoch: 2 [23040/54579 (42%)]\tLoss: 124.043243\n",
            "Train Epoch: 2 [24320/54579 (44%)]\tLoss: 124.259674\n",
            "Train Epoch: 2 [25600/54579 (47%)]\tLoss: 122.214500\n",
            "Train Epoch: 2 [26880/54579 (49%)]\tLoss: 124.822319\n",
            "Train Epoch: 2 [28160/54579 (52%)]\tLoss: 117.168594\n",
            "Train Epoch: 2 [29440/54579 (54%)]\tLoss: 120.787231\n",
            "Train Epoch: 2 [30720/54579 (56%)]\tLoss: 118.086464\n",
            "Train Epoch: 2 [32000/54579 (59%)]\tLoss: 122.410126\n",
            "Train Epoch: 2 [33280/54579 (61%)]\tLoss: 118.233154\n",
            "Train Epoch: 2 [34560/54579 (63%)]\tLoss: 121.532425\n",
            "Train Epoch: 2 [35840/54579 (66%)]\tLoss: 117.201485\n",
            "Train Epoch: 2 [37120/54579 (68%)]\tLoss: 117.568199\n",
            "Train Epoch: 2 [38400/54579 (70%)]\tLoss: 117.035645\n",
            "Train Epoch: 2 [39680/54579 (73%)]\tLoss: 120.521759\n",
            "Train Epoch: 2 [40960/54579 (75%)]\tLoss: 116.489433\n",
            "Train Epoch: 2 [42240/54579 (77%)]\tLoss: 117.974777\n",
            "Train Epoch: 2 [43520/54579 (80%)]\tLoss: 115.839310\n",
            "Train Epoch: 2 [44800/54579 (82%)]\tLoss: 112.024185\n",
            "Train Epoch: 2 [46080/54579 (84%)]\tLoss: 112.030167\n",
            "Train Epoch: 2 [47360/54579 (87%)]\tLoss: 119.575836\n",
            "Train Epoch: 2 [48640/54579 (89%)]\tLoss: 118.148582\n",
            "Train Epoch: 2 [49920/54579 (91%)]\tLoss: 114.057472\n",
            "Train Epoch: 2 [51200/54579 (94%)]\tLoss: 115.238457\n",
            "Train Epoch: 2 [52480/54579 (96%)]\tLoss: 117.733215\n",
            "Train Epoch: 2 [53760/54579 (98%)]\tLoss: 112.167969\n",
            "====> Epoch: 2 Average loss: 121.4035\n",
            "====> anomaly set loss: 150.7320\n",
            "Train Epoch: 3 [0/54579 (0%)]\tLoss: 114.289703\n",
            "Train Epoch: 3 [1280/54579 (2%)]\tLoss: 116.266426\n",
            "Train Epoch: 3 [2560/54579 (5%)]\tLoss: 112.876114\n",
            "Train Epoch: 3 [3840/54579 (7%)]\tLoss: 119.210220\n",
            "Train Epoch: 3 [5120/54579 (9%)]\tLoss: 114.729202\n",
            "Train Epoch: 3 [6400/54579 (12%)]\tLoss: 113.985138\n",
            "Train Epoch: 3 [7680/54579 (14%)]\tLoss: 114.816925\n",
            "Train Epoch: 3 [8960/54579 (16%)]\tLoss: 122.274139\n",
            "Train Epoch: 3 [10240/54579 (19%)]\tLoss: 115.400154\n",
            "Train Epoch: 3 [11520/54579 (21%)]\tLoss: 112.867950\n",
            "Train Epoch: 3 [12800/54579 (23%)]\tLoss: 113.164810\n",
            "Train Epoch: 3 [14080/54579 (26%)]\tLoss: 115.443878\n",
            "Train Epoch: 3 [15360/54579 (28%)]\tLoss: 113.352768\n",
            "Train Epoch: 3 [16640/54579 (30%)]\tLoss: 112.151939\n",
            "Train Epoch: 3 [17920/54579 (33%)]\tLoss: 114.624725\n",
            "Train Epoch: 3 [19200/54579 (35%)]\tLoss: 113.242767\n",
            "Train Epoch: 3 [20480/54579 (37%)]\tLoss: 114.406868\n",
            "Train Epoch: 3 [21760/54579 (40%)]\tLoss: 113.318024\n",
            "Train Epoch: 3 [23040/54579 (42%)]\tLoss: 115.817841\n",
            "Train Epoch: 3 [24320/54579 (44%)]\tLoss: 112.441040\n",
            "Train Epoch: 3 [25600/54579 (47%)]\tLoss: 112.209763\n",
            "Train Epoch: 3 [26880/54579 (49%)]\tLoss: 112.041634\n",
            "Train Epoch: 3 [28160/54579 (52%)]\tLoss: 114.999557\n",
            "Train Epoch: 3 [29440/54579 (54%)]\tLoss: 111.211823\n",
            "Train Epoch: 3 [30720/54579 (56%)]\tLoss: 111.670509\n",
            "Train Epoch: 3 [32000/54579 (59%)]\tLoss: 114.950836\n",
            "Train Epoch: 3 [33280/54579 (61%)]\tLoss: 114.376602\n",
            "Train Epoch: 3 [34560/54579 (63%)]\tLoss: 116.807465\n",
            "Train Epoch: 3 [35840/54579 (66%)]\tLoss: 115.901901\n",
            "Train Epoch: 3 [37120/54579 (68%)]\tLoss: 115.764977\n",
            "Train Epoch: 3 [38400/54579 (70%)]\tLoss: 113.716003\n",
            "Train Epoch: 3 [39680/54579 (73%)]\tLoss: 114.154266\n",
            "Train Epoch: 3 [40960/54579 (75%)]\tLoss: 109.869659\n",
            "Train Epoch: 3 [42240/54579 (77%)]\tLoss: 114.529854\n",
            "Train Epoch: 3 [43520/54579 (80%)]\tLoss: 112.280853\n",
            "Train Epoch: 3 [44800/54579 (82%)]\tLoss: 116.781693\n",
            "Train Epoch: 3 [46080/54579 (84%)]\tLoss: 109.431015\n",
            "Train Epoch: 3 [47360/54579 (87%)]\tLoss: 108.738647\n",
            "Train Epoch: 3 [48640/54579 (89%)]\tLoss: 116.928505\n",
            "Train Epoch: 3 [49920/54579 (91%)]\tLoss: 113.614449\n",
            "Train Epoch: 3 [51200/54579 (94%)]\tLoss: 112.523628\n",
            "Train Epoch: 3 [52480/54579 (96%)]\tLoss: 107.001198\n",
            "Train Epoch: 3 [53760/54579 (98%)]\tLoss: 113.757462\n",
            "====> Epoch: 3 Average loss: 113.8281\n",
            "====> anomaly set loss: 144.3984\n",
            "Train Epoch: 4 [0/54579 (0%)]\tLoss: 111.775284\n",
            "Train Epoch: 4 [1280/54579 (2%)]\tLoss: 111.393066\n",
            "Train Epoch: 4 [2560/54579 (5%)]\tLoss: 111.947258\n",
            "Train Epoch: 4 [3840/54579 (7%)]\tLoss: 111.209534\n",
            "Train Epoch: 4 [5120/54579 (9%)]\tLoss: 114.520065\n",
            "Train Epoch: 4 [6400/54579 (12%)]\tLoss: 112.586685\n",
            "Train Epoch: 4 [7680/54579 (14%)]\tLoss: 109.206497\n",
            "Train Epoch: 4 [8960/54579 (16%)]\tLoss: 115.765404\n",
            "Train Epoch: 4 [10240/54579 (19%)]\tLoss: 109.710793\n",
            "Train Epoch: 4 [11520/54579 (21%)]\tLoss: 108.924622\n",
            "Train Epoch: 4 [12800/54579 (23%)]\tLoss: 109.557068\n",
            "Train Epoch: 4 [14080/54579 (26%)]\tLoss: 113.796669\n",
            "Train Epoch: 4 [15360/54579 (28%)]\tLoss: 112.663811\n",
            "Train Epoch: 4 [16640/54579 (30%)]\tLoss: 109.419258\n",
            "Train Epoch: 4 [17920/54579 (33%)]\tLoss: 110.254974\n",
            "Train Epoch: 4 [19200/54579 (35%)]\tLoss: 115.415848\n",
            "Train Epoch: 4 [20480/54579 (37%)]\tLoss: 108.209045\n",
            "Train Epoch: 4 [21760/54579 (40%)]\tLoss: 110.728409\n",
            "Train Epoch: 4 [23040/54579 (42%)]\tLoss: 106.784409\n",
            "Train Epoch: 4 [24320/54579 (44%)]\tLoss: 109.185326\n",
            "Train Epoch: 4 [25600/54579 (47%)]\tLoss: 113.058006\n",
            "Train Epoch: 4 [26880/54579 (49%)]\tLoss: 111.857193\n",
            "Train Epoch: 4 [28160/54579 (52%)]\tLoss: 115.485146\n",
            "Train Epoch: 4 [29440/54579 (54%)]\tLoss: 107.986610\n",
            "Train Epoch: 4 [30720/54579 (56%)]\tLoss: 113.402817\n",
            "Train Epoch: 4 [32000/54579 (59%)]\tLoss: 113.751320\n",
            "Train Epoch: 4 [33280/54579 (61%)]\tLoss: 110.716286\n",
            "Train Epoch: 4 [34560/54579 (63%)]\tLoss: 112.272247\n",
            "Train Epoch: 4 [35840/54579 (66%)]\tLoss: 111.820969\n",
            "Train Epoch: 4 [37120/54579 (68%)]\tLoss: 110.758522\n",
            "Train Epoch: 4 [38400/54579 (70%)]\tLoss: 108.036270\n",
            "Train Epoch: 4 [39680/54579 (73%)]\tLoss: 107.933716\n",
            "Train Epoch: 4 [40960/54579 (75%)]\tLoss: 110.029190\n",
            "Train Epoch: 4 [42240/54579 (77%)]\tLoss: 113.667084\n",
            "Train Epoch: 4 [43520/54579 (80%)]\tLoss: 110.803123\n",
            "Train Epoch: 4 [44800/54579 (82%)]\tLoss: 111.241348\n",
            "Train Epoch: 4 [46080/54579 (84%)]\tLoss: 110.247360\n",
            "Train Epoch: 4 [47360/54579 (87%)]\tLoss: 108.529724\n",
            "Train Epoch: 4 [48640/54579 (89%)]\tLoss: 113.400459\n",
            "Train Epoch: 4 [49920/54579 (91%)]\tLoss: 114.757767\n",
            "Train Epoch: 4 [51200/54579 (94%)]\tLoss: 108.373077\n",
            "Train Epoch: 4 [52480/54579 (96%)]\tLoss: 113.148903\n",
            "Train Epoch: 4 [53760/54579 (98%)]\tLoss: 110.288353\n",
            "====> Epoch: 4 Average loss: 110.7180\n",
            "====> anomaly set loss: 141.5064\n",
            "Train Epoch: 5 [0/54579 (0%)]\tLoss: 107.951797\n",
            "Train Epoch: 5 [1280/54579 (2%)]\tLoss: 107.645782\n",
            "Train Epoch: 5 [2560/54579 (5%)]\tLoss: 106.078499\n",
            "Train Epoch: 5 [3840/54579 (7%)]\tLoss: 109.187897\n",
            "Train Epoch: 5 [5120/54579 (9%)]\tLoss: 108.070084\n",
            "Train Epoch: 5 [6400/54579 (12%)]\tLoss: 110.551353\n",
            "Train Epoch: 5 [7680/54579 (14%)]\tLoss: 112.569931\n",
            "Train Epoch: 5 [8960/54579 (16%)]\tLoss: 112.289879\n",
            "Train Epoch: 5 [10240/54579 (19%)]\tLoss: 106.736176\n",
            "Train Epoch: 5 [11520/54579 (21%)]\tLoss: 110.697044\n",
            "Train Epoch: 5 [12800/54579 (23%)]\tLoss: 114.167801\n",
            "Train Epoch: 5 [14080/54579 (26%)]\tLoss: 110.714806\n",
            "Train Epoch: 5 [15360/54579 (28%)]\tLoss: 113.493790\n",
            "Train Epoch: 5 [16640/54579 (30%)]\tLoss: 109.815338\n",
            "Train Epoch: 5 [17920/54579 (33%)]\tLoss: 108.159424\n",
            "Train Epoch: 5 [19200/54579 (35%)]\tLoss: 112.081345\n",
            "Train Epoch: 5 [20480/54579 (37%)]\tLoss: 105.981873\n",
            "Train Epoch: 5 [21760/54579 (40%)]\tLoss: 106.847725\n",
            "Train Epoch: 5 [23040/54579 (42%)]\tLoss: 109.009460\n",
            "Train Epoch: 5 [24320/54579 (44%)]\tLoss: 108.346962\n",
            "Train Epoch: 5 [25600/54579 (47%)]\tLoss: 113.218117\n",
            "Train Epoch: 5 [26880/54579 (49%)]\tLoss: 110.390823\n",
            "Train Epoch: 5 [28160/54579 (52%)]\tLoss: 112.328140\n",
            "Train Epoch: 5 [29440/54579 (54%)]\tLoss: 106.109985\n",
            "Train Epoch: 5 [30720/54579 (56%)]\tLoss: 109.194008\n",
            "Train Epoch: 5 [32000/54579 (59%)]\tLoss: 104.935577\n",
            "Train Epoch: 5 [33280/54579 (61%)]\tLoss: 110.469398\n",
            "Train Epoch: 5 [34560/54579 (63%)]\tLoss: 111.807671\n",
            "Train Epoch: 5 [35840/54579 (66%)]\tLoss: 107.762138\n",
            "Train Epoch: 5 [37120/54579 (68%)]\tLoss: 109.877777\n",
            "Train Epoch: 5 [38400/54579 (70%)]\tLoss: 107.001297\n",
            "Train Epoch: 5 [39680/54579 (73%)]\tLoss: 104.523125\n",
            "Train Epoch: 5 [40960/54579 (75%)]\tLoss: 108.629944\n",
            "Train Epoch: 5 [42240/54579 (77%)]\tLoss: 105.689995\n",
            "Train Epoch: 5 [43520/54579 (80%)]\tLoss: 107.492241\n",
            "Train Epoch: 5 [44800/54579 (82%)]\tLoss: 108.958275\n",
            "Train Epoch: 5 [46080/54579 (84%)]\tLoss: 104.798370\n",
            "Train Epoch: 5 [47360/54579 (87%)]\tLoss: 108.800323\n",
            "Train Epoch: 5 [48640/54579 (89%)]\tLoss: 110.154564\n",
            "Train Epoch: 5 [49920/54579 (91%)]\tLoss: 110.848099\n",
            "Train Epoch: 5 [51200/54579 (94%)]\tLoss: 112.045593\n",
            "Train Epoch: 5 [52480/54579 (96%)]\tLoss: 106.373993\n",
            "Train Epoch: 5 [53760/54579 (98%)]\tLoss: 104.710327\n",
            "====> Epoch: 5 Average loss: 108.7892\n",
            "====> anomaly set loss: 139.1644\n",
            "Train Epoch: 6 [0/54579 (0%)]\tLoss: 107.865768\n",
            "Train Epoch: 6 [1280/54579 (2%)]\tLoss: 107.794830\n",
            "Train Epoch: 6 [2560/54579 (5%)]\tLoss: 110.168137\n",
            "Train Epoch: 6 [3840/54579 (7%)]\tLoss: 106.915634\n",
            "Train Epoch: 6 [5120/54579 (9%)]\tLoss: 108.082649\n",
            "Train Epoch: 6 [6400/54579 (12%)]\tLoss: 106.204971\n",
            "Train Epoch: 6 [7680/54579 (14%)]\tLoss: 106.344696\n",
            "Train Epoch: 6 [8960/54579 (16%)]\tLoss: 108.002594\n",
            "Train Epoch: 6 [10240/54579 (19%)]\tLoss: 110.038338\n",
            "Train Epoch: 6 [11520/54579 (21%)]\tLoss: 108.765121\n",
            "Train Epoch: 6 [12800/54579 (23%)]\tLoss: 111.688751\n",
            "Train Epoch: 6 [14080/54579 (26%)]\tLoss: 105.899033\n",
            "Train Epoch: 6 [15360/54579 (28%)]\tLoss: 106.452538\n",
            "Train Epoch: 6 [16640/54579 (30%)]\tLoss: 111.697006\n",
            "Train Epoch: 6 [17920/54579 (33%)]\tLoss: 107.745377\n",
            "Train Epoch: 6 [19200/54579 (35%)]\tLoss: 106.064423\n",
            "Train Epoch: 6 [20480/54579 (37%)]\tLoss: 108.995110\n",
            "Train Epoch: 6 [21760/54579 (40%)]\tLoss: 107.125786\n",
            "Train Epoch: 6 [23040/54579 (42%)]\tLoss: 106.870056\n",
            "Train Epoch: 6 [24320/54579 (44%)]\tLoss: 107.833000\n",
            "Train Epoch: 6 [25600/54579 (47%)]\tLoss: 102.438309\n",
            "Train Epoch: 6 [26880/54579 (49%)]\tLoss: 106.330917\n",
            "Train Epoch: 6 [28160/54579 (52%)]\tLoss: 106.037529\n",
            "Train Epoch: 6 [29440/54579 (54%)]\tLoss: 107.956680\n",
            "Train Epoch: 6 [30720/54579 (56%)]\tLoss: 110.336174\n",
            "Train Epoch: 6 [32000/54579 (59%)]\tLoss: 104.594078\n",
            "Train Epoch: 6 [33280/54579 (61%)]\tLoss: 106.711617\n",
            "Train Epoch: 6 [34560/54579 (63%)]\tLoss: 107.848618\n",
            "Train Epoch: 6 [35840/54579 (66%)]\tLoss: 108.641716\n",
            "Train Epoch: 6 [37120/54579 (68%)]\tLoss: 106.533440\n",
            "Train Epoch: 6 [38400/54579 (70%)]\tLoss: 107.617508\n",
            "Train Epoch: 6 [39680/54579 (73%)]\tLoss: 103.095497\n",
            "Train Epoch: 6 [40960/54579 (75%)]\tLoss: 108.290833\n",
            "Train Epoch: 6 [42240/54579 (77%)]\tLoss: 108.808411\n",
            "Train Epoch: 6 [43520/54579 (80%)]\tLoss: 107.936859\n",
            "Train Epoch: 6 [44800/54579 (82%)]\tLoss: 110.610794\n",
            "Train Epoch: 6 [46080/54579 (84%)]\tLoss: 105.793579\n",
            "Train Epoch: 6 [47360/54579 (87%)]\tLoss: 108.318909\n",
            "Train Epoch: 6 [48640/54579 (89%)]\tLoss: 110.446777\n",
            "Train Epoch: 6 [49920/54579 (91%)]\tLoss: 107.166046\n",
            "Train Epoch: 6 [51200/54579 (94%)]\tLoss: 108.878159\n",
            "Train Epoch: 6 [52480/54579 (96%)]\tLoss: 106.298019\n",
            "Train Epoch: 6 [53760/54579 (98%)]\tLoss: 108.437897\n",
            "====> Epoch: 6 Average loss: 107.4908\n",
            "====> anomaly set loss: 137.7347\n",
            "Train Epoch: 7 [0/54579 (0%)]\tLoss: 105.546234\n",
            "Train Epoch: 7 [1280/54579 (2%)]\tLoss: 105.009819\n",
            "Train Epoch: 7 [2560/54579 (5%)]\tLoss: 109.804604\n",
            "Train Epoch: 7 [3840/54579 (7%)]\tLoss: 106.037857\n",
            "Train Epoch: 7 [5120/54579 (9%)]\tLoss: 103.124481\n",
            "Train Epoch: 7 [6400/54579 (12%)]\tLoss: 106.037369\n",
            "Train Epoch: 7 [7680/54579 (14%)]\tLoss: 102.691872\n",
            "Train Epoch: 7 [8960/54579 (16%)]\tLoss: 104.811035\n",
            "Train Epoch: 7 [10240/54579 (19%)]\tLoss: 105.637306\n",
            "Train Epoch: 7 [11520/54579 (21%)]\tLoss: 104.501518\n",
            "Train Epoch: 7 [12800/54579 (23%)]\tLoss: 107.553040\n",
            "Train Epoch: 7 [14080/54579 (26%)]\tLoss: 107.063751\n",
            "Train Epoch: 7 [15360/54579 (28%)]\tLoss: 109.121704\n",
            "Train Epoch: 7 [16640/54579 (30%)]\tLoss: 103.459274\n",
            "Train Epoch: 7 [17920/54579 (33%)]\tLoss: 99.766792\n",
            "Train Epoch: 7 [19200/54579 (35%)]\tLoss: 107.991341\n",
            "Train Epoch: 7 [20480/54579 (37%)]\tLoss: 107.583252\n",
            "Train Epoch: 7 [21760/54579 (40%)]\tLoss: 104.849609\n",
            "Train Epoch: 7 [23040/54579 (42%)]\tLoss: 103.442078\n",
            "Train Epoch: 7 [24320/54579 (44%)]\tLoss: 103.554596\n",
            "Train Epoch: 7 [25600/54579 (47%)]\tLoss: 105.128128\n",
            "Train Epoch: 7 [26880/54579 (49%)]\tLoss: 104.853668\n",
            "Train Epoch: 7 [28160/54579 (52%)]\tLoss: 105.379082\n",
            "Train Epoch: 7 [29440/54579 (54%)]\tLoss: 106.207291\n",
            "Train Epoch: 7 [30720/54579 (56%)]\tLoss: 107.512329\n",
            "Train Epoch: 7 [32000/54579 (59%)]\tLoss: 105.727440\n",
            "Train Epoch: 7 [33280/54579 (61%)]\tLoss: 102.732445\n",
            "Train Epoch: 7 [34560/54579 (63%)]\tLoss: 106.318001\n",
            "Train Epoch: 7 [35840/54579 (66%)]\tLoss: 107.850868\n",
            "Train Epoch: 7 [37120/54579 (68%)]\tLoss: 104.003265\n",
            "Train Epoch: 7 [38400/54579 (70%)]\tLoss: 103.550896\n",
            "Train Epoch: 7 [39680/54579 (73%)]\tLoss: 106.105827\n",
            "Train Epoch: 7 [40960/54579 (75%)]\tLoss: 108.330124\n",
            "Train Epoch: 7 [42240/54579 (77%)]\tLoss: 107.365952\n",
            "Train Epoch: 7 [43520/54579 (80%)]\tLoss: 104.064087\n",
            "Train Epoch: 7 [44800/54579 (82%)]\tLoss: 104.375023\n",
            "Train Epoch: 7 [46080/54579 (84%)]\tLoss: 104.953934\n",
            "Train Epoch: 7 [47360/54579 (87%)]\tLoss: 107.183517\n",
            "Train Epoch: 7 [48640/54579 (89%)]\tLoss: 102.943542\n",
            "Train Epoch: 7 [49920/54579 (91%)]\tLoss: 107.552330\n",
            "Train Epoch: 7 [51200/54579 (94%)]\tLoss: 107.692177\n",
            "Train Epoch: 7 [52480/54579 (96%)]\tLoss: 106.730881\n",
            "Train Epoch: 7 [53760/54579 (98%)]\tLoss: 106.808624\n",
            "====> Epoch: 7 Average loss: 106.5626\n",
            "====> anomaly set loss: 135.9020\n",
            "Train Epoch: 8 [0/54579 (0%)]\tLoss: 108.076767\n",
            "Train Epoch: 8 [1280/54579 (2%)]\tLoss: 107.620850\n",
            "Train Epoch: 8 [2560/54579 (5%)]\tLoss: 107.634239\n",
            "Train Epoch: 8 [3840/54579 (7%)]\tLoss: 107.415001\n",
            "Train Epoch: 8 [5120/54579 (9%)]\tLoss: 104.079346\n",
            "Train Epoch: 8 [6400/54579 (12%)]\tLoss: 106.504501\n",
            "Train Epoch: 8 [7680/54579 (14%)]\tLoss: 108.220955\n",
            "Train Epoch: 8 [8960/54579 (16%)]\tLoss: 104.700775\n",
            "Train Epoch: 8 [10240/54579 (19%)]\tLoss: 106.532303\n",
            "Train Epoch: 8 [11520/54579 (21%)]\tLoss: 109.588791\n",
            "Train Epoch: 8 [12800/54579 (23%)]\tLoss: 104.875153\n",
            "Train Epoch: 8 [14080/54579 (26%)]\tLoss: 103.540100\n",
            "Train Epoch: 8 [15360/54579 (28%)]\tLoss: 107.485657\n",
            "Train Epoch: 8 [16640/54579 (30%)]\tLoss: 106.711914\n",
            "Train Epoch: 8 [17920/54579 (33%)]\tLoss: 102.755951\n",
            "Train Epoch: 8 [19200/54579 (35%)]\tLoss: 104.798218\n",
            "Train Epoch: 8 [20480/54579 (37%)]\tLoss: 108.691559\n",
            "Train Epoch: 8 [21760/54579 (40%)]\tLoss: 105.716660\n",
            "Train Epoch: 8 [23040/54579 (42%)]\tLoss: 107.113693\n",
            "Train Epoch: 8 [24320/54579 (44%)]\tLoss: 101.967087\n",
            "Train Epoch: 8 [25600/54579 (47%)]\tLoss: 103.083519\n",
            "Train Epoch: 8 [26880/54579 (49%)]\tLoss: 107.800957\n",
            "Train Epoch: 8 [28160/54579 (52%)]\tLoss: 109.008072\n",
            "Train Epoch: 8 [29440/54579 (54%)]\tLoss: 105.062248\n",
            "Train Epoch: 8 [30720/54579 (56%)]\tLoss: 105.986359\n",
            "Train Epoch: 8 [32000/54579 (59%)]\tLoss: 104.759262\n",
            "Train Epoch: 8 [33280/54579 (61%)]\tLoss: 103.079025\n",
            "Train Epoch: 8 [34560/54579 (63%)]\tLoss: 104.993233\n",
            "Train Epoch: 8 [35840/54579 (66%)]\tLoss: 107.888458\n",
            "Train Epoch: 8 [37120/54579 (68%)]\tLoss: 102.650833\n",
            "Train Epoch: 8 [38400/54579 (70%)]\tLoss: 104.521271\n",
            "Train Epoch: 8 [39680/54579 (73%)]\tLoss: 106.348183\n",
            "Train Epoch: 8 [40960/54579 (75%)]\tLoss: 105.977570\n",
            "Train Epoch: 8 [42240/54579 (77%)]\tLoss: 106.524567\n",
            "Train Epoch: 8 [43520/54579 (80%)]\tLoss: 109.124184\n",
            "Train Epoch: 8 [44800/54579 (82%)]\tLoss: 109.130959\n",
            "Train Epoch: 8 [46080/54579 (84%)]\tLoss: 103.768875\n",
            "Train Epoch: 8 [47360/54579 (87%)]\tLoss: 106.250488\n",
            "Train Epoch: 8 [48640/54579 (89%)]\tLoss: 111.923973\n",
            "Train Epoch: 8 [49920/54579 (91%)]\tLoss: 102.227806\n",
            "Train Epoch: 8 [51200/54579 (94%)]\tLoss: 110.435028\n",
            "Train Epoch: 8 [52480/54579 (96%)]\tLoss: 101.573952\n",
            "Train Epoch: 8 [53760/54579 (98%)]\tLoss: 103.853119\n",
            "====> Epoch: 8 Average loss: 105.8731\n",
            "====> anomaly set loss: 135.0725\n",
            "Train Epoch: 9 [0/54579 (0%)]\tLoss: 102.939499\n",
            "Train Epoch: 9 [1280/54579 (2%)]\tLoss: 105.680122\n",
            "Train Epoch: 9 [2560/54579 (5%)]\tLoss: 104.883545\n",
            "Train Epoch: 9 [3840/54579 (7%)]\tLoss: 103.637093\n",
            "Train Epoch: 9 [5120/54579 (9%)]\tLoss: 105.369972\n",
            "Train Epoch: 9 [6400/54579 (12%)]\tLoss: 103.829643\n",
            "Train Epoch: 9 [7680/54579 (14%)]\tLoss: 105.336700\n",
            "Train Epoch: 9 [8960/54579 (16%)]\tLoss: 110.628029\n",
            "Train Epoch: 9 [10240/54579 (19%)]\tLoss: 106.760735\n",
            "Train Epoch: 9 [11520/54579 (21%)]\tLoss: 102.502243\n",
            "Train Epoch: 9 [12800/54579 (23%)]\tLoss: 108.207077\n",
            "Train Epoch: 9 [14080/54579 (26%)]\tLoss: 103.650864\n",
            "Train Epoch: 9 [15360/54579 (28%)]\tLoss: 100.945763\n",
            "Train Epoch: 9 [16640/54579 (30%)]\tLoss: 105.579941\n",
            "Train Epoch: 9 [17920/54579 (33%)]\tLoss: 104.963737\n",
            "Train Epoch: 9 [19200/54579 (35%)]\tLoss: 107.001236\n",
            "Train Epoch: 9 [20480/54579 (37%)]\tLoss: 106.487900\n",
            "Train Epoch: 9 [21760/54579 (40%)]\tLoss: 105.168533\n",
            "Train Epoch: 9 [23040/54579 (42%)]\tLoss: 105.609863\n",
            "Train Epoch: 9 [24320/54579 (44%)]\tLoss: 106.549286\n",
            "Train Epoch: 9 [25600/54579 (47%)]\tLoss: 100.768143\n",
            "Train Epoch: 9 [26880/54579 (49%)]\tLoss: 105.560669\n",
            "Train Epoch: 9 [28160/54579 (52%)]\tLoss: 105.577484\n",
            "Train Epoch: 9 [29440/54579 (54%)]\tLoss: 107.656540\n",
            "Train Epoch: 9 [30720/54579 (56%)]\tLoss: 103.368576\n",
            "Train Epoch: 9 [32000/54579 (59%)]\tLoss: 106.039322\n",
            "Train Epoch: 9 [33280/54579 (61%)]\tLoss: 104.368141\n",
            "Train Epoch: 9 [34560/54579 (63%)]\tLoss: 102.864914\n",
            "Train Epoch: 9 [35840/54579 (66%)]\tLoss: 102.126320\n",
            "Train Epoch: 9 [37120/54579 (68%)]\tLoss: 106.541183\n",
            "Train Epoch: 9 [38400/54579 (70%)]\tLoss: 102.653954\n",
            "Train Epoch: 9 [39680/54579 (73%)]\tLoss: 102.555557\n",
            "Train Epoch: 9 [40960/54579 (75%)]\tLoss: 107.777657\n",
            "Train Epoch: 9 [42240/54579 (77%)]\tLoss: 102.165466\n",
            "Train Epoch: 9 [43520/54579 (80%)]\tLoss: 102.188560\n",
            "Train Epoch: 9 [44800/54579 (82%)]\tLoss: 105.448166\n",
            "Train Epoch: 9 [46080/54579 (84%)]\tLoss: 103.741272\n",
            "Train Epoch: 9 [47360/54579 (87%)]\tLoss: 105.458374\n",
            "Train Epoch: 9 [48640/54579 (89%)]\tLoss: 101.951302\n",
            "Train Epoch: 9 [49920/54579 (91%)]\tLoss: 110.368149\n",
            "Train Epoch: 9 [51200/54579 (94%)]\tLoss: 107.429886\n",
            "Train Epoch: 9 [52480/54579 (96%)]\tLoss: 104.543571\n",
            "Train Epoch: 9 [53760/54579 (98%)]\tLoss: 103.565567\n",
            "====> Epoch: 9 Average loss: 105.3122\n",
            "====> anomaly set loss: 135.9509\n",
            "Train Epoch: 10 [0/54579 (0%)]\tLoss: 111.017792\n",
            "Train Epoch: 10 [1280/54579 (2%)]\tLoss: 104.171021\n",
            "Train Epoch: 10 [2560/54579 (5%)]\tLoss: 102.452133\n",
            "Train Epoch: 10 [3840/54579 (7%)]\tLoss: 103.636627\n",
            "Train Epoch: 10 [5120/54579 (9%)]\tLoss: 103.907616\n",
            "Train Epoch: 10 [6400/54579 (12%)]\tLoss: 106.532684\n",
            "Train Epoch: 10 [7680/54579 (14%)]\tLoss: 105.067108\n",
            "Train Epoch: 10 [8960/54579 (16%)]\tLoss: 104.024246\n",
            "Train Epoch: 10 [10240/54579 (19%)]\tLoss: 104.982086\n",
            "Train Epoch: 10 [11520/54579 (21%)]\tLoss: 105.237244\n",
            "Train Epoch: 10 [12800/54579 (23%)]\tLoss: 104.069725\n",
            "Train Epoch: 10 [14080/54579 (26%)]\tLoss: 103.179863\n",
            "Train Epoch: 10 [15360/54579 (28%)]\tLoss: 101.755936\n",
            "Train Epoch: 10 [16640/54579 (30%)]\tLoss: 105.952103\n",
            "Train Epoch: 10 [17920/54579 (33%)]\tLoss: 108.565758\n",
            "Train Epoch: 10 [19200/54579 (35%)]\tLoss: 105.350937\n",
            "Train Epoch: 10 [20480/54579 (37%)]\tLoss: 103.322693\n",
            "Train Epoch: 10 [21760/54579 (40%)]\tLoss: 100.612106\n",
            "Train Epoch: 10 [23040/54579 (42%)]\tLoss: 104.259079\n",
            "Train Epoch: 10 [24320/54579 (44%)]\tLoss: 108.454391\n",
            "Train Epoch: 10 [25600/54579 (47%)]\tLoss: 101.672653\n",
            "Train Epoch: 10 [26880/54579 (49%)]\tLoss: 107.797638\n",
            "Train Epoch: 10 [28160/54579 (52%)]\tLoss: 101.150482\n",
            "Train Epoch: 10 [29440/54579 (54%)]\tLoss: 104.777023\n",
            "Train Epoch: 10 [30720/54579 (56%)]\tLoss: 107.016594\n",
            "Train Epoch: 10 [32000/54579 (59%)]\tLoss: 105.071030\n",
            "Train Epoch: 10 [33280/54579 (61%)]\tLoss: 104.209045\n",
            "Train Epoch: 10 [34560/54579 (63%)]\tLoss: 101.410156\n",
            "Train Epoch: 10 [35840/54579 (66%)]\tLoss: 105.136795\n",
            "Train Epoch: 10 [37120/54579 (68%)]\tLoss: 102.161751\n",
            "Train Epoch: 10 [38400/54579 (70%)]\tLoss: 99.480736\n",
            "Train Epoch: 10 [39680/54579 (73%)]\tLoss: 102.859940\n",
            "Train Epoch: 10 [40960/54579 (75%)]\tLoss: 106.109482\n",
            "Train Epoch: 10 [42240/54579 (77%)]\tLoss: 108.404121\n",
            "Train Epoch: 10 [43520/54579 (80%)]\tLoss: 104.466949\n",
            "Train Epoch: 10 [44800/54579 (82%)]\tLoss: 101.315369\n",
            "Train Epoch: 10 [46080/54579 (84%)]\tLoss: 103.978737\n",
            "Train Epoch: 10 [47360/54579 (87%)]\tLoss: 106.958130\n",
            "Train Epoch: 10 [48640/54579 (89%)]\tLoss: 102.913971\n",
            "Train Epoch: 10 [49920/54579 (91%)]\tLoss: 105.669220\n",
            "Train Epoch: 10 [51200/54579 (94%)]\tLoss: 102.255554\n",
            "Train Epoch: 10 [52480/54579 (96%)]\tLoss: 100.977715\n",
            "Train Epoch: 10 [53760/54579 (98%)]\tLoss: 101.947433\n",
            "====> Epoch: 10 Average loss: 104.8999\n",
            "====> anomaly set loss: 134.0636\n",
            "Train Epoch: 11 [0/54579 (0%)]\tLoss: 107.077469\n",
            "Train Epoch: 11 [1280/54579 (2%)]\tLoss: 104.104004\n",
            "Train Epoch: 11 [2560/54579 (5%)]\tLoss: 101.817726\n",
            "Train Epoch: 11 [3840/54579 (7%)]\tLoss: 106.226784\n",
            "Train Epoch: 11 [5120/54579 (9%)]\tLoss: 102.973244\n",
            "Train Epoch: 11 [6400/54579 (12%)]\tLoss: 105.101341\n",
            "Train Epoch: 11 [7680/54579 (14%)]\tLoss: 107.742996\n",
            "Train Epoch: 11 [8960/54579 (16%)]\tLoss: 111.787827\n",
            "Train Epoch: 11 [10240/54579 (19%)]\tLoss: 106.660072\n",
            "Train Epoch: 11 [11520/54579 (21%)]\tLoss: 106.240723\n",
            "Train Epoch: 11 [12800/54579 (23%)]\tLoss: 105.973259\n",
            "Train Epoch: 11 [14080/54579 (26%)]\tLoss: 105.511909\n",
            "Train Epoch: 11 [15360/54579 (28%)]\tLoss: 103.638016\n",
            "Train Epoch: 11 [16640/54579 (30%)]\tLoss: 107.966751\n",
            "Train Epoch: 11 [17920/54579 (33%)]\tLoss: 103.727448\n",
            "Train Epoch: 11 [19200/54579 (35%)]\tLoss: 107.968697\n",
            "Train Epoch: 11 [20480/54579 (37%)]\tLoss: 107.231186\n",
            "Train Epoch: 11 [21760/54579 (40%)]\tLoss: 103.309349\n",
            "Train Epoch: 11 [23040/54579 (42%)]\tLoss: 106.479080\n",
            "Train Epoch: 11 [24320/54579 (44%)]\tLoss: 104.988441\n",
            "Train Epoch: 11 [25600/54579 (47%)]\tLoss: 101.796013\n",
            "Train Epoch: 11 [26880/54579 (49%)]\tLoss: 102.607986\n",
            "Train Epoch: 11 [28160/54579 (52%)]\tLoss: 107.081375\n",
            "Train Epoch: 11 [29440/54579 (54%)]\tLoss: 104.108368\n",
            "Train Epoch: 11 [30720/54579 (56%)]\tLoss: 103.834442\n",
            "Train Epoch: 11 [32000/54579 (59%)]\tLoss: 102.960831\n",
            "Train Epoch: 11 [33280/54579 (61%)]\tLoss: 108.395554\n",
            "Train Epoch: 11 [34560/54579 (63%)]\tLoss: 103.086983\n",
            "Train Epoch: 11 [35840/54579 (66%)]\tLoss: 101.630043\n",
            "Train Epoch: 11 [37120/54579 (68%)]\tLoss: 103.164017\n",
            "Train Epoch: 11 [38400/54579 (70%)]\tLoss: 105.674362\n",
            "Train Epoch: 11 [39680/54579 (73%)]\tLoss: 101.707962\n",
            "Train Epoch: 11 [40960/54579 (75%)]\tLoss: 99.858398\n",
            "Train Epoch: 11 [42240/54579 (77%)]\tLoss: 105.168747\n",
            "Train Epoch: 11 [43520/54579 (80%)]\tLoss: 108.811935\n",
            "Train Epoch: 11 [44800/54579 (82%)]\tLoss: 105.578094\n",
            "Train Epoch: 11 [46080/54579 (84%)]\tLoss: 104.651001\n",
            "Train Epoch: 11 [47360/54579 (87%)]\tLoss: 106.070793\n",
            "Train Epoch: 11 [48640/54579 (89%)]\tLoss: 105.002228\n",
            "Train Epoch: 11 [49920/54579 (91%)]\tLoss: 108.665390\n",
            "Train Epoch: 11 [51200/54579 (94%)]\tLoss: 104.235779\n",
            "Train Epoch: 11 [52480/54579 (96%)]\tLoss: 102.554672\n",
            "Train Epoch: 11 [53760/54579 (98%)]\tLoss: 102.008789\n",
            "====> Epoch: 11 Average loss: 104.5313\n",
            "====> anomaly set loss: 134.5376\n",
            "Train Epoch: 12 [0/54579 (0%)]\tLoss: 102.680969\n",
            "Train Epoch: 12 [1280/54579 (2%)]\tLoss: 103.031723\n",
            "Train Epoch: 12 [2560/54579 (5%)]\tLoss: 101.427841\n",
            "Train Epoch: 12 [3840/54579 (7%)]\tLoss: 105.723808\n",
            "Train Epoch: 12 [5120/54579 (9%)]\tLoss: 103.162445\n",
            "Train Epoch: 12 [6400/54579 (12%)]\tLoss: 104.279556\n",
            "Train Epoch: 12 [7680/54579 (14%)]\tLoss: 107.526093\n",
            "Train Epoch: 12 [8960/54579 (16%)]\tLoss: 104.262909\n",
            "Train Epoch: 12 [10240/54579 (19%)]\tLoss: 103.763794\n",
            "Train Epoch: 12 [11520/54579 (21%)]\tLoss: 101.312256\n",
            "Train Epoch: 12 [12800/54579 (23%)]\tLoss: 102.286346\n",
            "Train Epoch: 12 [14080/54579 (26%)]\tLoss: 106.189102\n",
            "Train Epoch: 12 [15360/54579 (28%)]\tLoss: 102.308830\n",
            "Train Epoch: 12 [16640/54579 (30%)]\tLoss: 102.987198\n",
            "Train Epoch: 12 [17920/54579 (33%)]\tLoss: 105.660957\n",
            "Train Epoch: 12 [19200/54579 (35%)]\tLoss: 100.937790\n",
            "Train Epoch: 12 [20480/54579 (37%)]\tLoss: 104.896622\n",
            "Train Epoch: 12 [21760/54579 (40%)]\tLoss: 108.750511\n",
            "Train Epoch: 12 [23040/54579 (42%)]\tLoss: 105.943443\n",
            "Train Epoch: 12 [24320/54579 (44%)]\tLoss: 105.529037\n",
            "Train Epoch: 12 [25600/54579 (47%)]\tLoss: 103.647659\n",
            "Train Epoch: 12 [26880/54579 (49%)]\tLoss: 102.020782\n",
            "Train Epoch: 12 [28160/54579 (52%)]\tLoss: 106.018143\n",
            "Train Epoch: 12 [29440/54579 (54%)]\tLoss: 101.550888\n",
            "Train Epoch: 12 [30720/54579 (56%)]\tLoss: 103.682968\n",
            "Train Epoch: 12 [32000/54579 (59%)]\tLoss: 101.870438\n",
            "Train Epoch: 12 [33280/54579 (61%)]\tLoss: 99.408714\n",
            "Train Epoch: 12 [34560/54579 (63%)]\tLoss: 101.265518\n",
            "Train Epoch: 12 [35840/54579 (66%)]\tLoss: 105.375015\n",
            "Train Epoch: 12 [37120/54579 (68%)]\tLoss: 103.653473\n",
            "Train Epoch: 12 [38400/54579 (70%)]\tLoss: 102.872299\n",
            "Train Epoch: 12 [39680/54579 (73%)]\tLoss: 105.426888\n",
            "Train Epoch: 12 [40960/54579 (75%)]\tLoss: 101.215050\n",
            "Train Epoch: 12 [42240/54579 (77%)]\tLoss: 100.964035\n",
            "Train Epoch: 12 [43520/54579 (80%)]\tLoss: 102.380692\n",
            "Train Epoch: 12 [44800/54579 (82%)]\tLoss: 105.681770\n",
            "Train Epoch: 12 [46080/54579 (84%)]\tLoss: 102.853073\n",
            "Train Epoch: 12 [47360/54579 (87%)]\tLoss: 101.614990\n",
            "Train Epoch: 12 [48640/54579 (89%)]\tLoss: 106.441963\n",
            "Train Epoch: 12 [49920/54579 (91%)]\tLoss: 105.497765\n",
            "Train Epoch: 12 [51200/54579 (94%)]\tLoss: 110.541672\n",
            "Train Epoch: 12 [52480/54579 (96%)]\tLoss: 104.509308\n",
            "Train Epoch: 12 [53760/54579 (98%)]\tLoss: 104.803627\n",
            "====> Epoch: 12 Average loss: 104.1562\n",
            "====> anomaly set loss: 134.5274\n",
            "Train Epoch: 13 [0/54579 (0%)]\tLoss: 103.237671\n",
            "Train Epoch: 13 [1280/54579 (2%)]\tLoss: 101.991234\n",
            "Train Epoch: 13 [2560/54579 (5%)]\tLoss: 100.482803\n",
            "Train Epoch: 13 [3840/54579 (7%)]\tLoss: 104.293114\n",
            "Train Epoch: 13 [5120/54579 (9%)]\tLoss: 101.193451\n",
            "Train Epoch: 13 [6400/54579 (12%)]\tLoss: 105.085831\n",
            "Train Epoch: 13 [7680/54579 (14%)]\tLoss: 99.615936\n",
            "Train Epoch: 13 [8960/54579 (16%)]\tLoss: 98.519775\n",
            "Train Epoch: 13 [10240/54579 (19%)]\tLoss: 111.638367\n",
            "Train Epoch: 13 [11520/54579 (21%)]\tLoss: 103.036362\n",
            "Train Epoch: 13 [12800/54579 (23%)]\tLoss: 102.979095\n",
            "Train Epoch: 13 [14080/54579 (26%)]\tLoss: 102.622337\n",
            "Train Epoch: 13 [15360/54579 (28%)]\tLoss: 104.568550\n",
            "Train Epoch: 13 [16640/54579 (30%)]\tLoss: 105.472404\n",
            "Train Epoch: 13 [17920/54579 (33%)]\tLoss: 102.983810\n",
            "Train Epoch: 13 [19200/54579 (35%)]\tLoss: 104.657623\n",
            "Train Epoch: 13 [20480/54579 (37%)]\tLoss: 106.407898\n",
            "Train Epoch: 13 [21760/54579 (40%)]\tLoss: 107.727058\n",
            "Train Epoch: 13 [23040/54579 (42%)]\tLoss: 106.196312\n",
            "Train Epoch: 13 [24320/54579 (44%)]\tLoss: 102.546394\n",
            "Train Epoch: 13 [25600/54579 (47%)]\tLoss: 106.349777\n",
            "Train Epoch: 13 [26880/54579 (49%)]\tLoss: 102.400490\n",
            "Train Epoch: 13 [28160/54579 (52%)]\tLoss: 106.082275\n",
            "Train Epoch: 13 [29440/54579 (54%)]\tLoss: 105.109749\n",
            "Train Epoch: 13 [30720/54579 (56%)]\tLoss: 99.082291\n",
            "Train Epoch: 13 [32000/54579 (59%)]\tLoss: 99.499672\n",
            "Train Epoch: 13 [33280/54579 (61%)]\tLoss: 106.755035\n",
            "Train Epoch: 13 [34560/54579 (63%)]\tLoss: 105.186447\n",
            "Train Epoch: 13 [35840/54579 (66%)]\tLoss: 106.979721\n",
            "Train Epoch: 13 [37120/54579 (68%)]\tLoss: 104.346542\n",
            "Train Epoch: 13 [38400/54579 (70%)]\tLoss: 102.013710\n",
            "Train Epoch: 13 [39680/54579 (73%)]\tLoss: 103.985710\n",
            "Train Epoch: 13 [40960/54579 (75%)]\tLoss: 101.480965\n",
            "Train Epoch: 13 [42240/54579 (77%)]\tLoss: 105.460968\n",
            "Train Epoch: 13 [43520/54579 (80%)]\tLoss: 102.114479\n",
            "Train Epoch: 13 [44800/54579 (82%)]\tLoss: 104.153931\n",
            "Train Epoch: 13 [46080/54579 (84%)]\tLoss: 104.542923\n",
            "Train Epoch: 13 [47360/54579 (87%)]\tLoss: 106.200241\n",
            "Train Epoch: 13 [48640/54579 (89%)]\tLoss: 106.424843\n",
            "Train Epoch: 13 [49920/54579 (91%)]\tLoss: 103.415054\n",
            "Train Epoch: 13 [51200/54579 (94%)]\tLoss: 103.241669\n",
            "Train Epoch: 13 [52480/54579 (96%)]\tLoss: 105.002625\n",
            "Train Epoch: 13 [53760/54579 (98%)]\tLoss: 103.901039\n",
            "====> Epoch: 13 Average loss: 103.9639\n",
            "====> anomaly set loss: 133.8407\n",
            "Train Epoch: 14 [0/54579 (0%)]\tLoss: 102.647171\n",
            "Train Epoch: 14 [1280/54579 (2%)]\tLoss: 105.686935\n",
            "Train Epoch: 14 [2560/54579 (5%)]\tLoss: 96.727722\n",
            "Train Epoch: 14 [3840/54579 (7%)]\tLoss: 105.736359\n",
            "Train Epoch: 14 [5120/54579 (9%)]\tLoss: 103.635254\n",
            "Train Epoch: 14 [6400/54579 (12%)]\tLoss: 101.410057\n",
            "Train Epoch: 14 [7680/54579 (14%)]\tLoss: 102.595520\n",
            "Train Epoch: 14 [8960/54579 (16%)]\tLoss: 104.297638\n",
            "Train Epoch: 14 [10240/54579 (19%)]\tLoss: 105.940643\n",
            "Train Epoch: 14 [11520/54579 (21%)]\tLoss: 105.793839\n",
            "Train Epoch: 14 [12800/54579 (23%)]\tLoss: 108.794373\n",
            "Train Epoch: 14 [14080/54579 (26%)]\tLoss: 104.420425\n",
            "Train Epoch: 14 [15360/54579 (28%)]\tLoss: 106.452164\n",
            "Train Epoch: 14 [16640/54579 (30%)]\tLoss: 104.164879\n",
            "Train Epoch: 14 [17920/54579 (33%)]\tLoss: 101.874725\n",
            "Train Epoch: 14 [19200/54579 (35%)]\tLoss: 105.861969\n",
            "Train Epoch: 14 [20480/54579 (37%)]\tLoss: 101.542267\n",
            "Train Epoch: 14 [21760/54579 (40%)]\tLoss: 105.150055\n",
            "Train Epoch: 14 [23040/54579 (42%)]\tLoss: 106.337601\n",
            "Train Epoch: 14 [24320/54579 (44%)]\tLoss: 102.997620\n",
            "Train Epoch: 14 [25600/54579 (47%)]\tLoss: 103.680099\n",
            "Train Epoch: 14 [26880/54579 (49%)]\tLoss: 104.579643\n",
            "Train Epoch: 14 [28160/54579 (52%)]\tLoss: 102.440948\n",
            "Train Epoch: 14 [29440/54579 (54%)]\tLoss: 107.880920\n",
            "Train Epoch: 14 [30720/54579 (56%)]\tLoss: 104.925880\n",
            "Train Epoch: 14 [32000/54579 (59%)]\tLoss: 99.545349\n",
            "Train Epoch: 14 [33280/54579 (61%)]\tLoss: 104.055748\n",
            "Train Epoch: 14 [34560/54579 (63%)]\tLoss: 104.910934\n",
            "Train Epoch: 14 [35840/54579 (66%)]\tLoss: 105.558685\n",
            "Train Epoch: 14 [37120/54579 (68%)]\tLoss: 105.159958\n",
            "Train Epoch: 14 [38400/54579 (70%)]\tLoss: 102.383347\n",
            "Train Epoch: 14 [39680/54579 (73%)]\tLoss: 103.030121\n",
            "Train Epoch: 14 [40960/54579 (75%)]\tLoss: 100.482430\n",
            "Train Epoch: 14 [42240/54579 (77%)]\tLoss: 108.057648\n",
            "Train Epoch: 14 [43520/54579 (80%)]\tLoss: 104.143051\n",
            "Train Epoch: 14 [44800/54579 (82%)]\tLoss: 103.881409\n",
            "Train Epoch: 14 [46080/54579 (84%)]\tLoss: 105.269287\n",
            "Train Epoch: 14 [47360/54579 (87%)]\tLoss: 105.986710\n",
            "Train Epoch: 14 [48640/54579 (89%)]\tLoss: 101.941299\n",
            "Train Epoch: 14 [49920/54579 (91%)]\tLoss: 102.437042\n",
            "Train Epoch: 14 [51200/54579 (94%)]\tLoss: 105.310760\n",
            "Train Epoch: 14 [52480/54579 (96%)]\tLoss: 103.551964\n",
            "Train Epoch: 14 [53760/54579 (98%)]\tLoss: 102.413773\n",
            "====> Epoch: 14 Average loss: 103.6231\n",
            "====> anomaly set loss: 134.3866\n",
            "Train Epoch: 15 [0/54579 (0%)]\tLoss: 104.644501\n",
            "Train Epoch: 15 [1280/54579 (2%)]\tLoss: 100.289841\n",
            "Train Epoch: 15 [2560/54579 (5%)]\tLoss: 108.131615\n",
            "Train Epoch: 15 [3840/54579 (7%)]\tLoss: 101.608711\n",
            "Train Epoch: 15 [5120/54579 (9%)]\tLoss: 104.814125\n",
            "Train Epoch: 15 [6400/54579 (12%)]\tLoss: 102.220474\n",
            "Train Epoch: 15 [7680/54579 (14%)]\tLoss: 100.269455\n",
            "Train Epoch: 15 [8960/54579 (16%)]\tLoss: 101.650764\n",
            "Train Epoch: 15 [10240/54579 (19%)]\tLoss: 103.257385\n",
            "Train Epoch: 15 [11520/54579 (21%)]\tLoss: 101.184944\n",
            "Train Epoch: 15 [12800/54579 (23%)]\tLoss: 101.919495\n",
            "Train Epoch: 15 [14080/54579 (26%)]\tLoss: 103.180153\n",
            "Train Epoch: 15 [15360/54579 (28%)]\tLoss: 104.315697\n",
            "Train Epoch: 15 [16640/54579 (30%)]\tLoss: 105.383148\n",
            "Train Epoch: 15 [17920/54579 (33%)]\tLoss: 103.002319\n",
            "Train Epoch: 15 [19200/54579 (35%)]\tLoss: 100.425392\n",
            "Train Epoch: 15 [20480/54579 (37%)]\tLoss: 101.777298\n",
            "Train Epoch: 15 [21760/54579 (40%)]\tLoss: 105.394722\n",
            "Train Epoch: 15 [23040/54579 (42%)]\tLoss: 102.897568\n",
            "Train Epoch: 15 [24320/54579 (44%)]\tLoss: 102.451729\n",
            "Train Epoch: 15 [25600/54579 (47%)]\tLoss: 106.699211\n",
            "Train Epoch: 15 [26880/54579 (49%)]\tLoss: 103.272919\n",
            "Train Epoch: 15 [28160/54579 (52%)]\tLoss: 103.819687\n",
            "Train Epoch: 15 [29440/54579 (54%)]\tLoss: 104.432693\n",
            "Train Epoch: 15 [30720/54579 (56%)]\tLoss: 102.131737\n",
            "Train Epoch: 15 [32000/54579 (59%)]\tLoss: 106.963478\n",
            "Train Epoch: 15 [33280/54579 (61%)]\tLoss: 102.404999\n",
            "Train Epoch: 15 [34560/54579 (63%)]\tLoss: 104.301544\n",
            "Train Epoch: 15 [35840/54579 (66%)]\tLoss: 97.349129\n",
            "Train Epoch: 15 [37120/54579 (68%)]\tLoss: 102.741257\n",
            "Train Epoch: 15 [38400/54579 (70%)]\tLoss: 104.676453\n",
            "Train Epoch: 15 [39680/54579 (73%)]\tLoss: 102.667465\n",
            "Train Epoch: 15 [40960/54579 (75%)]\tLoss: 102.624725\n",
            "Train Epoch: 15 [42240/54579 (77%)]\tLoss: 103.088692\n",
            "Train Epoch: 15 [43520/54579 (80%)]\tLoss: 102.560112\n",
            "Train Epoch: 15 [44800/54579 (82%)]\tLoss: 108.246719\n",
            "Train Epoch: 15 [46080/54579 (84%)]\tLoss: 100.086800\n",
            "Train Epoch: 15 [47360/54579 (87%)]\tLoss: 107.828995\n",
            "Train Epoch: 15 [48640/54579 (89%)]\tLoss: 104.860237\n",
            "Train Epoch: 15 [49920/54579 (91%)]\tLoss: 101.776115\n",
            "Train Epoch: 15 [51200/54579 (94%)]\tLoss: 98.319992\n",
            "Train Epoch: 15 [52480/54579 (96%)]\tLoss: 103.562469\n",
            "Train Epoch: 15 [53760/54579 (98%)]\tLoss: 104.224602\n",
            "====> Epoch: 15 Average loss: 103.4290\n",
            "====> anomaly set loss: 133.5079\n",
            "Train Epoch: 16 [0/54579 (0%)]\tLoss: 100.960358\n",
            "Train Epoch: 16 [1280/54579 (2%)]\tLoss: 98.896744\n",
            "Train Epoch: 16 [2560/54579 (5%)]\tLoss: 104.319725\n",
            "Train Epoch: 16 [3840/54579 (7%)]\tLoss: 104.164604\n",
            "Train Epoch: 16 [5120/54579 (9%)]\tLoss: 104.377975\n",
            "Train Epoch: 16 [6400/54579 (12%)]\tLoss: 104.378052\n",
            "Train Epoch: 16 [7680/54579 (14%)]\tLoss: 103.094887\n",
            "Train Epoch: 16 [8960/54579 (16%)]\tLoss: 103.821426\n",
            "Train Epoch: 16 [10240/54579 (19%)]\tLoss: 103.147568\n",
            "Train Epoch: 16 [11520/54579 (21%)]\tLoss: 102.034515\n",
            "Train Epoch: 16 [12800/54579 (23%)]\tLoss: 102.895691\n",
            "Train Epoch: 16 [14080/54579 (26%)]\tLoss: 102.934937\n",
            "Train Epoch: 16 [15360/54579 (28%)]\tLoss: 103.847412\n",
            "Train Epoch: 16 [16640/54579 (30%)]\tLoss: 102.683876\n",
            "Train Epoch: 16 [17920/54579 (33%)]\tLoss: 103.367302\n",
            "Train Epoch: 16 [19200/54579 (35%)]\tLoss: 105.530319\n",
            "Train Epoch: 16 [20480/54579 (37%)]\tLoss: 103.440544\n",
            "Train Epoch: 16 [21760/54579 (40%)]\tLoss: 101.296280\n",
            "Train Epoch: 16 [23040/54579 (42%)]\tLoss: 101.818100\n",
            "Train Epoch: 16 [24320/54579 (44%)]\tLoss: 104.054428\n",
            "Train Epoch: 16 [25600/54579 (47%)]\tLoss: 101.364166\n",
            "Train Epoch: 16 [26880/54579 (49%)]\tLoss: 99.181770\n",
            "Train Epoch: 16 [28160/54579 (52%)]\tLoss: 103.686234\n",
            "Train Epoch: 16 [29440/54579 (54%)]\tLoss: 105.706955\n",
            "Train Epoch: 16 [30720/54579 (56%)]\tLoss: 105.426895\n",
            "Train Epoch: 16 [32000/54579 (59%)]\tLoss: 103.130814\n",
            "Train Epoch: 16 [33280/54579 (61%)]\tLoss: 102.381119\n",
            "Train Epoch: 16 [34560/54579 (63%)]\tLoss: 103.650734\n",
            "Train Epoch: 16 [35840/54579 (66%)]\tLoss: 103.985359\n",
            "Train Epoch: 16 [37120/54579 (68%)]\tLoss: 104.708618\n",
            "Train Epoch: 16 [38400/54579 (70%)]\tLoss: 98.907990\n",
            "Train Epoch: 16 [39680/54579 (73%)]\tLoss: 103.043465\n",
            "Train Epoch: 16 [40960/54579 (75%)]\tLoss: 103.482788\n",
            "Train Epoch: 16 [42240/54579 (77%)]\tLoss: 103.180862\n",
            "Train Epoch: 16 [43520/54579 (80%)]\tLoss: 101.577614\n",
            "Train Epoch: 16 [44800/54579 (82%)]\tLoss: 103.939682\n",
            "Train Epoch: 16 [46080/54579 (84%)]\tLoss: 103.194695\n",
            "Train Epoch: 16 [47360/54579 (87%)]\tLoss: 103.129082\n",
            "Train Epoch: 16 [48640/54579 (89%)]\tLoss: 102.986679\n",
            "Train Epoch: 16 [49920/54579 (91%)]\tLoss: 100.894897\n",
            "Train Epoch: 16 [51200/54579 (94%)]\tLoss: 98.800369\n",
            "Train Epoch: 16 [52480/54579 (96%)]\tLoss: 103.038834\n",
            "Train Epoch: 16 [53760/54579 (98%)]\tLoss: 101.974030\n",
            "====> Epoch: 16 Average loss: 103.2106\n",
            "====> anomaly set loss: 133.2584\n",
            "Train Epoch: 17 [0/54579 (0%)]\tLoss: 99.683640\n",
            "Train Epoch: 17 [1280/54579 (2%)]\tLoss: 99.619507\n",
            "Train Epoch: 17 [2560/54579 (5%)]\tLoss: 102.180542\n",
            "Train Epoch: 17 [3840/54579 (7%)]\tLoss: 98.952553\n",
            "Train Epoch: 17 [5120/54579 (9%)]\tLoss: 103.091934\n",
            "Train Epoch: 17 [6400/54579 (12%)]\tLoss: 103.193253\n",
            "Train Epoch: 17 [7680/54579 (14%)]\tLoss: 99.847038\n",
            "Train Epoch: 17 [8960/54579 (16%)]\tLoss: 103.946533\n",
            "Train Epoch: 17 [10240/54579 (19%)]\tLoss: 103.126465\n",
            "Train Epoch: 17 [11520/54579 (21%)]\tLoss: 100.252846\n",
            "Train Epoch: 17 [12800/54579 (23%)]\tLoss: 102.702194\n",
            "Train Epoch: 17 [14080/54579 (26%)]\tLoss: 101.891464\n",
            "Train Epoch: 17 [15360/54579 (28%)]\tLoss: 103.651932\n",
            "Train Epoch: 17 [16640/54579 (30%)]\tLoss: 104.134628\n",
            "Train Epoch: 17 [17920/54579 (33%)]\tLoss: 105.456276\n",
            "Train Epoch: 17 [19200/54579 (35%)]\tLoss: 105.254196\n",
            "Train Epoch: 17 [20480/54579 (37%)]\tLoss: 102.124329\n",
            "Train Epoch: 17 [21760/54579 (40%)]\tLoss: 107.884872\n",
            "Train Epoch: 17 [23040/54579 (42%)]\tLoss: 100.704300\n",
            "Train Epoch: 17 [24320/54579 (44%)]\tLoss: 103.675781\n",
            "Train Epoch: 17 [25600/54579 (47%)]\tLoss: 102.531754\n",
            "Train Epoch: 17 [26880/54579 (49%)]\tLoss: 108.145432\n",
            "Train Epoch: 17 [28160/54579 (52%)]\tLoss: 105.289238\n",
            "Train Epoch: 17 [29440/54579 (54%)]\tLoss: 98.592995\n",
            "Train Epoch: 17 [30720/54579 (56%)]\tLoss: 102.992714\n",
            "Train Epoch: 17 [32000/54579 (59%)]\tLoss: 101.674530\n",
            "Train Epoch: 17 [33280/54579 (61%)]\tLoss: 101.433121\n",
            "Train Epoch: 17 [34560/54579 (63%)]\tLoss: 107.320244\n",
            "Train Epoch: 17 [35840/54579 (66%)]\tLoss: 101.340240\n",
            "Train Epoch: 17 [37120/54579 (68%)]\tLoss: 103.248199\n",
            "Train Epoch: 17 [38400/54579 (70%)]\tLoss: 103.188911\n",
            "Train Epoch: 17 [39680/54579 (73%)]\tLoss: 103.224464\n",
            "Train Epoch: 17 [40960/54579 (75%)]\tLoss: 101.561699\n",
            "Train Epoch: 17 [42240/54579 (77%)]\tLoss: 105.150551\n",
            "Train Epoch: 17 [43520/54579 (80%)]\tLoss: 102.295502\n",
            "Train Epoch: 17 [44800/54579 (82%)]\tLoss: 101.364342\n",
            "Train Epoch: 17 [46080/54579 (84%)]\tLoss: 102.175362\n",
            "Train Epoch: 17 [47360/54579 (87%)]\tLoss: 98.645210\n",
            "Train Epoch: 17 [48640/54579 (89%)]\tLoss: 104.941925\n",
            "Train Epoch: 17 [49920/54579 (91%)]\tLoss: 105.203033\n",
            "Train Epoch: 17 [51200/54579 (94%)]\tLoss: 101.633743\n",
            "Train Epoch: 17 [52480/54579 (96%)]\tLoss: 106.157722\n",
            "Train Epoch: 17 [53760/54579 (98%)]\tLoss: 103.471581\n",
            "====> Epoch: 17 Average loss: 103.0138\n",
            "====> anomaly set loss: 133.7454\n",
            "Train Epoch: 18 [0/54579 (0%)]\tLoss: 102.546890\n",
            "Train Epoch: 18 [1280/54579 (2%)]\tLoss: 102.773643\n",
            "Train Epoch: 18 [2560/54579 (5%)]\tLoss: 100.080627\n",
            "Train Epoch: 18 [3840/54579 (7%)]\tLoss: 99.234451\n",
            "Train Epoch: 18 [5120/54579 (9%)]\tLoss: 103.043701\n",
            "Train Epoch: 18 [6400/54579 (12%)]\tLoss: 103.669357\n",
            "Train Epoch: 18 [7680/54579 (14%)]\tLoss: 102.290939\n",
            "Train Epoch: 18 [8960/54579 (16%)]\tLoss: 101.083984\n",
            "Train Epoch: 18 [10240/54579 (19%)]\tLoss: 99.735764\n",
            "Train Epoch: 18 [11520/54579 (21%)]\tLoss: 103.058449\n",
            "Train Epoch: 18 [12800/54579 (23%)]\tLoss: 102.001205\n",
            "Train Epoch: 18 [14080/54579 (26%)]\tLoss: 102.347084\n",
            "Train Epoch: 18 [15360/54579 (28%)]\tLoss: 101.317398\n",
            "Train Epoch: 18 [16640/54579 (30%)]\tLoss: 103.317314\n",
            "Train Epoch: 18 [17920/54579 (33%)]\tLoss: 100.721909\n",
            "Train Epoch: 18 [19200/54579 (35%)]\tLoss: 102.728210\n",
            "Train Epoch: 18 [20480/54579 (37%)]\tLoss: 102.557999\n",
            "Train Epoch: 18 [21760/54579 (40%)]\tLoss: 100.979431\n",
            "Train Epoch: 18 [23040/54579 (42%)]\tLoss: 100.613571\n",
            "Train Epoch: 18 [24320/54579 (44%)]\tLoss: 102.871323\n",
            "Train Epoch: 18 [25600/54579 (47%)]\tLoss: 101.551231\n",
            "Train Epoch: 18 [26880/54579 (49%)]\tLoss: 102.884567\n",
            "Train Epoch: 18 [28160/54579 (52%)]\tLoss: 99.544060\n",
            "Train Epoch: 18 [29440/54579 (54%)]\tLoss: 103.123589\n",
            "Train Epoch: 18 [30720/54579 (56%)]\tLoss: 105.695198\n",
            "Train Epoch: 18 [32000/54579 (59%)]\tLoss: 103.415085\n",
            "Train Epoch: 18 [33280/54579 (61%)]\tLoss: 100.036865\n",
            "Train Epoch: 18 [34560/54579 (63%)]\tLoss: 100.658104\n",
            "Train Epoch: 18 [35840/54579 (66%)]\tLoss: 101.709030\n",
            "Train Epoch: 18 [37120/54579 (68%)]\tLoss: 104.636765\n",
            "Train Epoch: 18 [38400/54579 (70%)]\tLoss: 104.773239\n",
            "Train Epoch: 18 [39680/54579 (73%)]\tLoss: 101.238251\n",
            "Train Epoch: 18 [40960/54579 (75%)]\tLoss: 104.102112\n",
            "Train Epoch: 18 [42240/54579 (77%)]\tLoss: 106.433250\n",
            "Train Epoch: 18 [43520/54579 (80%)]\tLoss: 102.770386\n",
            "Train Epoch: 18 [44800/54579 (82%)]\tLoss: 101.822769\n",
            "Train Epoch: 18 [46080/54579 (84%)]\tLoss: 105.205093\n",
            "Train Epoch: 18 [47360/54579 (87%)]\tLoss: 99.761513\n",
            "Train Epoch: 18 [48640/54579 (89%)]\tLoss: 106.431534\n",
            "Train Epoch: 18 [49920/54579 (91%)]\tLoss: 104.728973\n",
            "Train Epoch: 18 [51200/54579 (94%)]\tLoss: 103.349739\n",
            "Train Epoch: 18 [52480/54579 (96%)]\tLoss: 106.051651\n",
            "Train Epoch: 18 [53760/54579 (98%)]\tLoss: 105.271408\n",
            "====> Epoch: 18 Average loss: 102.8740\n",
            "====> anomaly set loss: 132.6904\n",
            "Train Epoch: 19 [0/54579 (0%)]\tLoss: 102.546310\n",
            "Train Epoch: 19 [1280/54579 (2%)]\tLoss: 102.347321\n",
            "Train Epoch: 19 [2560/54579 (5%)]\tLoss: 102.960037\n",
            "Train Epoch: 19 [3840/54579 (7%)]\tLoss: 101.082695\n",
            "Train Epoch: 19 [5120/54579 (9%)]\tLoss: 107.081985\n",
            "Train Epoch: 19 [6400/54579 (12%)]\tLoss: 105.097710\n",
            "Train Epoch: 19 [7680/54579 (14%)]\tLoss: 101.286423\n",
            "Train Epoch: 19 [8960/54579 (16%)]\tLoss: 101.050407\n",
            "Train Epoch: 19 [10240/54579 (19%)]\tLoss: 99.669357\n",
            "Train Epoch: 19 [11520/54579 (21%)]\tLoss: 104.406532\n",
            "Train Epoch: 19 [12800/54579 (23%)]\tLoss: 103.744484\n",
            "Train Epoch: 19 [14080/54579 (26%)]\tLoss: 102.444679\n",
            "Train Epoch: 19 [15360/54579 (28%)]\tLoss: 105.152603\n",
            "Train Epoch: 19 [16640/54579 (30%)]\tLoss: 98.875664\n",
            "Train Epoch: 19 [17920/54579 (33%)]\tLoss: 106.219353\n",
            "Train Epoch: 19 [19200/54579 (35%)]\tLoss: 103.492790\n",
            "Train Epoch: 19 [20480/54579 (37%)]\tLoss: 103.044510\n",
            "Train Epoch: 19 [21760/54579 (40%)]\tLoss: 102.829727\n",
            "Train Epoch: 19 [23040/54579 (42%)]\tLoss: 103.593628\n",
            "Train Epoch: 19 [24320/54579 (44%)]\tLoss: 101.364449\n",
            "Train Epoch: 19 [25600/54579 (47%)]\tLoss: 102.604614\n",
            "Train Epoch: 19 [26880/54579 (49%)]\tLoss: 103.847122\n",
            "Train Epoch: 19 [28160/54579 (52%)]\tLoss: 102.615784\n",
            "Train Epoch: 19 [29440/54579 (54%)]\tLoss: 108.095253\n",
            "Train Epoch: 19 [30720/54579 (56%)]\tLoss: 100.143204\n",
            "Train Epoch: 19 [32000/54579 (59%)]\tLoss: 98.643417\n",
            "Train Epoch: 19 [33280/54579 (61%)]\tLoss: 99.553284\n",
            "Train Epoch: 19 [34560/54579 (63%)]\tLoss: 105.326141\n",
            "Train Epoch: 19 [35840/54579 (66%)]\tLoss: 101.437843\n",
            "Train Epoch: 19 [37120/54579 (68%)]\tLoss: 102.269775\n",
            "Train Epoch: 19 [38400/54579 (70%)]\tLoss: 103.274414\n",
            "Train Epoch: 19 [39680/54579 (73%)]\tLoss: 108.189384\n",
            "Train Epoch: 19 [40960/54579 (75%)]\tLoss: 102.436768\n",
            "Train Epoch: 19 [42240/54579 (77%)]\tLoss: 102.479042\n",
            "Train Epoch: 19 [43520/54579 (80%)]\tLoss: 104.965675\n",
            "Train Epoch: 19 [44800/54579 (82%)]\tLoss: 100.435677\n",
            "Train Epoch: 19 [46080/54579 (84%)]\tLoss: 101.650047\n",
            "Train Epoch: 19 [47360/54579 (87%)]\tLoss: 99.822556\n",
            "Train Epoch: 19 [48640/54579 (89%)]\tLoss: 102.208572\n",
            "Train Epoch: 19 [49920/54579 (91%)]\tLoss: 104.821991\n",
            "Train Epoch: 19 [51200/54579 (94%)]\tLoss: 103.738770\n",
            "Train Epoch: 19 [52480/54579 (96%)]\tLoss: 105.039047\n",
            "Train Epoch: 19 [53760/54579 (98%)]\tLoss: 106.479088\n",
            "====> Epoch: 19 Average loss: 102.7491\n",
            "====> anomaly set loss: 133.4402\n",
            "Train Epoch: 20 [0/54579 (0%)]\tLoss: 103.825256\n",
            "Train Epoch: 20 [1280/54579 (2%)]\tLoss: 100.351189\n",
            "Train Epoch: 20 [2560/54579 (5%)]\tLoss: 101.273170\n",
            "Train Epoch: 20 [3840/54579 (7%)]\tLoss: 102.106277\n",
            "Train Epoch: 20 [5120/54579 (9%)]\tLoss: 101.288223\n",
            "Train Epoch: 20 [6400/54579 (12%)]\tLoss: 103.880066\n",
            "Train Epoch: 20 [7680/54579 (14%)]\tLoss: 104.080376\n",
            "Train Epoch: 20 [8960/54579 (16%)]\tLoss: 96.804100\n",
            "Train Epoch: 20 [10240/54579 (19%)]\tLoss: 103.601746\n",
            "Train Epoch: 20 [11520/54579 (21%)]\tLoss: 99.658279\n",
            "Train Epoch: 20 [12800/54579 (23%)]\tLoss: 100.892395\n",
            "Train Epoch: 20 [14080/54579 (26%)]\tLoss: 102.899742\n",
            "Train Epoch: 20 [15360/54579 (28%)]\tLoss: 100.846779\n",
            "Train Epoch: 20 [16640/54579 (30%)]\tLoss: 104.561874\n",
            "Train Epoch: 20 [17920/54579 (33%)]\tLoss: 95.074715\n",
            "Train Epoch: 20 [19200/54579 (35%)]\tLoss: 103.857231\n",
            "Train Epoch: 20 [20480/54579 (37%)]\tLoss: 106.720184\n",
            "Train Epoch: 20 [21760/54579 (40%)]\tLoss: 104.344933\n",
            "Train Epoch: 20 [23040/54579 (42%)]\tLoss: 105.363983\n",
            "Train Epoch: 20 [24320/54579 (44%)]\tLoss: 100.911102\n",
            "Train Epoch: 20 [25600/54579 (47%)]\tLoss: 100.310394\n",
            "Train Epoch: 20 [26880/54579 (49%)]\tLoss: 109.989922\n",
            "Train Epoch: 20 [28160/54579 (52%)]\tLoss: 101.820229\n",
            "Train Epoch: 20 [29440/54579 (54%)]\tLoss: 102.506981\n",
            "Train Epoch: 20 [30720/54579 (56%)]\tLoss: 102.450760\n",
            "Train Epoch: 20 [32000/54579 (59%)]\tLoss: 105.162682\n",
            "Train Epoch: 20 [33280/54579 (61%)]\tLoss: 103.338074\n",
            "Train Epoch: 20 [34560/54579 (63%)]\tLoss: 105.956680\n",
            "Train Epoch: 20 [35840/54579 (66%)]\tLoss: 104.767487\n",
            "Train Epoch: 20 [37120/54579 (68%)]\tLoss: 103.385201\n",
            "Train Epoch: 20 [38400/54579 (70%)]\tLoss: 99.762344\n",
            "Train Epoch: 20 [39680/54579 (73%)]\tLoss: 100.284653\n",
            "Train Epoch: 20 [40960/54579 (75%)]\tLoss: 96.696716\n",
            "Train Epoch: 20 [42240/54579 (77%)]\tLoss: 104.597122\n",
            "Train Epoch: 20 [43520/54579 (80%)]\tLoss: 102.991241\n",
            "Train Epoch: 20 [44800/54579 (82%)]\tLoss: 100.569138\n",
            "Train Epoch: 20 [46080/54579 (84%)]\tLoss: 107.537033\n",
            "Train Epoch: 20 [47360/54579 (87%)]\tLoss: 105.385284\n",
            "Train Epoch: 20 [48640/54579 (89%)]\tLoss: 102.832687\n",
            "Train Epoch: 20 [49920/54579 (91%)]\tLoss: 96.716782\n",
            "Train Epoch: 20 [51200/54579 (94%)]\tLoss: 108.217384\n",
            "Train Epoch: 20 [52480/54579 (96%)]\tLoss: 96.700661\n",
            "Train Epoch: 20 [53760/54579 (98%)]\tLoss: 104.435272\n",
            "====> Epoch: 20 Average loss: 102.5674\n",
            "====> anomaly set loss: 133.8387\n",
            "AUC:0.83\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "os.makedirs(\"./recon\", exist_ok=True)\n",
        "\n",
        "# Anomaly number\n",
        "ANOMALY_TARGET = 5\n",
        "\n",
        "parser = argparse.ArgumentParser(description='VAE MNIST Example')\n",
        "parser.add_argument('--batch-size', type=int, default=128, metavar='N',\n",
        "                    help='input batch size for training (default: 128)')\n",
        "parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
        "                    help='number of epochs to train (default: 10)')\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                    help='disables CUDA training')\n",
        "parser.add_argument('--no-mps', action='store_true', default=False,\n",
        "                        help='disables macOS GPU training')\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
        "                    help='random seed (default: 1)')\n",
        "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                    help='how many batches to wait before logging training status')\n",
        "args = parser.parse_args(args=[])\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
        "use_mps = not args.no_mps and torch.backends.mps.is_available()\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "if args.cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "elif use_mps:\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "\n",
        "# Training dataset excluding anomaly target numbers\n",
        "train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transforms.ToTensor())\n",
        "train_mask = (train_dataset.targets != ANOMALY_TARGET)\n",
        "train_dataset.data = train_dataset.data[train_mask]\n",
        "train_dataset.targets = train_dataset.targets[train_mask]\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "all_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "# anomaly target numeric-only dataset\n",
        "anomaly_dataset = datasets.MNIST('../data', train=False, download=True, transform=transforms.ToTensor())\n",
        "anomaly_mask = (anomaly_dataset.targets == ANOMALY_TARGET)\n",
        "anomaly_dataset.data = anomaly_dataset.data[anomaly_mask]\n",
        "anomaly_dataset.targets = anomaly_dataset.targets[anomaly_mask]\n",
        "anomaly_loader = torch.utils.data.DataLoader(anomaly_dataset, batch_size=args.batch_size, shuffle=False, **kwargs)\n",
        "all_anomaly_loader = torch.utils.data.DataLoader(anomaly_dataset, batch_size=1, shuffle=False, **kwargs)\n",
        "\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "model = VAE().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return BCE + KLD\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "            \n",
        "        # Reconstruction for training data\n",
        "        if batch_idx == 0:\n",
        "                n = min(data.size(0), 8)\n",
        "                comparison = torch.cat([data[:n], recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
        "                save_image(comparison.cpu(), 'recon/recon_train' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "    return - (train_loss / len(train_loader.dataset))\n",
        "\n",
        "\n",
        "def anomaly(epoch):\n",
        "    model.eval()\n",
        "    anomaly_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, _) in enumerate(anomaly_loader):\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            anomaly_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "\n",
        "            # Reconstruction for anomaly data\n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 8)\n",
        "                comparison = torch.cat([data[:n], recon_batch.view(args.batch_size, 1, 28, 28)[:n]])\n",
        "                save_image(comparison.cpu(), 'recon/recon_anomaly' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "    anomaly_loss /= len(anomaly_loader.dataset)\n",
        "    print('====> anomaly set loss: {:.4f}'.format(anomaly_loss))\n",
        "\n",
        "    return - anomaly_loss\n",
        "\n",
        "def plot_roc():\n",
        "    y_true = np.concatenate([np.zeros(len(train_dataset)), np.ones(len(anomaly_dataset))])\n",
        "    y_score = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (data, _) in enumerate(all_train_loader):\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            train_score_loss = loss_function(recon_batch, data, mu, logvar)\n",
        "            train_score_loss = train_score_loss.cpu()\n",
        "            y_score.append(np.round(train_score_loss, 1).detach().numpy())\n",
        "        for i, (data, _) in enumerate(all_anomaly_loader):\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            anomaly_score_loss = loss_function(recon_batch, data, mu, logvar)\n",
        "            anomaly_score_loss = anomaly_score_loss.cpu()\n",
        "            y_score.append(np.round(anomaly_score_loss, 1).detach().numpy())\n",
        "\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score)\n",
        "    auc = metrics.auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr)\n",
        "    plt.xlabel('FPR: False positive rate', fontsize=13); plt.ylabel('TPR: True positive rate', fontsize=13)\n",
        "    plt.grid()\n",
        "    plt.savefig('./roc'+str(ANOMALY_TARGET)+'.png')\n",
        "    plt.close()\n",
        "    print(\"AUC:\" + str(np.round(auc, 2)))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fig1, ax1 = plt.subplots()\n",
        "    ax1.set_xlabel('Epoch', fontsize=15); ax1.set_ylabel('ELBO', fontsize=15)  \n",
        "    train_elbo_list = [];anomaly_loss_list = []\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        avg_train_elbo = train(epoch);avg_anomaly_elbo = anomaly(epoch)\n",
        "        train_elbo_list.append(avg_train_elbo);anomaly_loss_list.append(avg_anomaly_elbo)\n",
        "\n",
        "    # Plot ELBO\n",
        "    ax1.plot(np.arange(args.epochs), train_elbo_list, color=\"blue\", label=\"ELBO_Train\")\n",
        "    ax1.plot(np.arange(args.epochs), anomaly_loss_list, color=\"red\", label=\"ELBO_Anomaly\")\n",
        "    fig1.savefig('./elbo.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot ROC\n",
        "    plot_roc()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10 (default, Mar 13 2023, 10:26:41) \n[GCC 9.4.0]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}